{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "corrected-atlas",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-24T12:27:29.536389Z",
     "iopub.status.busy": "2021-08-24T12:27:29.535381Z",
     "iopub.status.idle": "2021-08-24T12:27:29.539867Z",
     "shell.execute_reply": "2021-08-24T12:27:29.539027Z"
    },
    "papermill": {
     "duration": 0.087944,
     "end_time": "2021-08-24T12:27:29.540098",
     "exception": false,
     "start_time": "2021-08-24T12:27:29.452154",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Add \"parameters\" (jupyter-notebook) tag to this cell, to allow papermill to inject different parameters\n",
    "from datetime import date\n",
    "it=0  #Iteration of gridsearch\n",
    "# to put it all in one folder by date, will be replaced by papermill\n",
    "today=date.today() \n",
    "rdate=today.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "marked-attribute",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-24T12:27:29.626347Z",
     "iopub.status.busy": "2021-08-24T12:27:29.625575Z",
     "iopub.status.idle": "2021-08-24T12:27:29.628943Z",
     "shell.execute_reply": "2021-08-24T12:27:29.629548Z"
    },
    "papermill": {
     "duration": 0.046275,
     "end_time": "2021-08-24T12:27:29.629721",
     "exception": false,
     "start_time": "2021-08-24T12:27:29.583446",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "it = 7\n",
    "rdate = \"2021-08-24\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alone-thong",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-24T12:27:29.698030Z",
     "iopub.status.busy": "2021-08-24T12:27:29.697426Z",
     "iopub.status.idle": "2021-08-24T12:27:29.702234Z",
     "shell.execute_reply": "2021-08-24T12:27:29.701733Z"
    },
    "papermill": {
     "duration": 0.040427,
     "end_time": "2021-08-24T12:27:29.702360",
     "exception": false,
     "start_time": "2021-08-24T12:27:29.661933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "# See if running on Colab (for setting the correct workdir and installing all dependencies)\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on CoLab')\n",
    "  g_colab = True\n",
    "else:\n",
    "  print('Not running on CoLab')\n",
    "  g_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-soviet",
   "metadata": {
    "id": "WF-aoH7jZLTn",
    "papermill": {
     "duration": 0.024614,
     "end_time": "2021-08-24T12:27:29.753748",
     "exception": false,
     "start_time": "2021-08-24T12:27:29.729134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "liquid-uruguay",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-08-24T12:27:29.820082Z",
     "iopub.status.busy": "2021-08-24T12:27:29.819542Z",
     "iopub.status.idle": "2021-08-24T12:27:30.532582Z",
     "shell.execute_reply": "2021-08-24T12:27:30.533686Z"
    },
    "id": "kTJohs4ytYsF",
    "outputId": "9bbe2ef6-3710-40fb-98aa-3fa94b3d69d3",
    "papermill": {
     "duration": 0.756793,
     "end_time": "2021-08-24T12:27:30.533996",
     "exception": false,
     "start_time": "2021-08-24T12:27:29.777203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "{'datasets': ['de_train_new', 'de_val_new', 'en_train_new', 'en_val_new'], 'model_name': 'xlm-roberta-base'}\n"
     ]
    }
   ],
   "source": [
    "# connect to drive\n",
    "if g_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/gdrive')\n",
    "    %cd \"/gdrive/MyDrive/1 Job/Product and Code/CogAlex 2.0/\"\n",
    "    \n",
    "# Choose model\n",
    "# Gridsearch parameters\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Original results with \"xlm_roberta_base\"\n",
    "grid = [{\"model_name\": [\"xlm-roberta-base\"],\n",
    "        \"datasets\": [[\"Nothing to see here\"], [\"de_train_new\", \"de_val_new\"], [\"de_train_new\"], [\"de_val_new\"],\n",
    "                      [\"en_train_new\", \"en_val_new\"], [\"en_train_new\"], [\"en_val_new\"], \n",
    "                      [\"de_train_new\", \"de_val_new\",\"en_train_new\", \"en_val_new\"]]},\n",
    "        {\"model_name\": [\"xlm-roberta-large\", \"distilbert-base-multilingual-cased\", \"bert-base-multilingual-uncased\", \"bert-base-multilingual-cased\", \"bert-base-cased\"],\n",
    "        \"datasets\": [[\"Nothing to see here\"], [\"de_train_new\", \"de_val_new\",\"en_train_new\", \"en_val_new\"]]}]\n",
    "\n",
    "pg = list(ParameterGrid(grid))\n",
    "print(len(pg))\n",
    "model_name = pg[it][\"model_name\"]\n",
    "datasets = pg[it][\"datasets\"]\n",
    "print(pg[it])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "attractive-finding",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-08-24T12:27:30.638438Z",
     "iopub.status.busy": "2021-08-24T12:27:30.637608Z",
     "iopub.status.idle": "2021-08-24T12:27:34.318247Z",
     "shell.execute_reply": "2021-08-24T12:27:34.317060Z"
    },
    "id": "3w3ZNBmItp_p",
    "outputId": "a925c761-08c7-49c7-816b-65fc1153a80a",
    "papermill": {
     "duration": 3.729634,
     "end_time": "2021-08-24T12:27:34.318523",
     "exception": false,
     "start_time": "2021-08-24T12:27:30.588889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (4.5.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from transformers) (2.25.1)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from transformers) (0.10.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from transformers) (4.55.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from transformers) (3.3.0)\r\n",
      "Requirement already satisfied: sacremoses in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from transformers) (0.0.43)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from transformers) (2020.11.13)\r\n",
      "Requirement already satisfied: dataclasses in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from transformers) (0.8)\r\n",
      "Requirement already satisfied: packaging in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from transformers) (20.8)\r\n",
      "Requirement already satisfied: filelock in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from transformers) (3.0.12)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from transformers) (1.19.4)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=0.5 in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.7.4.3)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from requests->transformers) (1.26.2)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from requests->transformers) (4.0.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from requests->transformers) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from requests->transformers) (2.10)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\r\n",
      "Requirement already satisfied: click in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\r\n",
      "Requirement already satisfied: joblib in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\r\n",
      "You should consider upgrading via the '/home/lv71502/clang/.virtualenvs/huggingface/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/lv71502/clang/.virtualenvs/huggingface/lib/python3.6/site-packages (0.1.94)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\r\n",
      "You should consider upgrading via the '/home/lv71502/clang/.virtualenvs/huggingface/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "premium-roommate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-24T12:27:34.457061Z",
     "iopub.status.busy": "2021-08-24T12:27:34.455414Z",
     "iopub.status.idle": "2021-08-24T12:27:36.067495Z",
     "shell.execute_reply": "2021-08-24T12:27:36.068583Z"
    },
    "id": "fM0lku9lt9As",
    "papermill": {
     "duration": 1.68967,
     "end_time": "2021-08-24T12:27:36.068983",
     "exception": true,
     "start_time": "2021-08-24T12:27:34.379313",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import torch                                              #for training the model\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "import pandas as pd                                       #for handling the data\n",
    "from transformers import XLMRobertaTokenizer, AutoTokenizer              #for loading the pretrained model and tokenizer\n",
    "from transformers import XLMRobertaForSequenceClassification, AutoModelForSequenceClassification\n",
    "from transformers import AdamW                            \n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn import preprocessing                         #for label encoding\n",
    "from sklearn.metrics import classification_report         #for showing performance on validation/test sets\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import ParameterGrid         #for grid search\n",
    "from sklearn.model_selection import ParameterSampler      #for random search\n",
    "from sklearn.utils.fixes import loguniform\n",
    "import sentencepiece\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "% matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-declaration",
   "metadata": {
    "id": "vXVdDSzPWpuQ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-patrol",
   "metadata": {
    "id": "4XjnVL2AfMrV",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Load data from text files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-romantic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uHTNIY9xM08",
    "outputId": "ba0503ea-8b49-4ed4-c343-0b6c2e19924e",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sk learn label encoder for changing the labels to integers\n",
    "labels=[\"ANT\", \"HYP\", \"RANDOM\", \"SYN\"]\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "#chinese \n",
    "data_train_zh = pd.read_csv('datasets/train_chinese_data.txt', sep=\"\\t\", header=None)\n",
    "data_train_zh.columns = [\"Word1\", \"Word2\", \"Label\"]\n",
    "\n",
    "data_valid_zh = pd.read_csv('datasets/validgold_chinese_data.txt', sep=\"\\t\", header=None)\n",
    "data_valid_zh.columns = [\"Word1\", \"Word2\", \"Label\"]\n",
    "\n",
    "data_train_zh[\"Label\"]=le.transform(data_train_zh[\"Label\"])\n",
    "data_valid_zh[\"Label\"]=le.transform(data_valid_zh[\"Label\"])\n",
    "\n",
    "#english\n",
    "if \"en_train_new\" in datasets:\n",
    "    print(\"EN Train: NEW\")\n",
    "    data_train_en = pd.read_csv('datasets/train_english_data_new.txt', sep=\"\\t\", header=None, usecols=[0,1,2])\n",
    "    data_train_en.columns = [\"Word1\", \"Word2\", \"Label\"]\n",
    "else:\n",
    "    print(\"EN Train: OLD\")\n",
    "    data_train_en = pd.read_csv('datasets/train_english_data.txt', sep=\"\\t\", header=None)\n",
    "    data_train_en.columns = [\"Word1\", \"Word2\", \"Label\"]\n",
    "\n",
    "if \"en_valid_new\" in datasets:\n",
    "    print(\"EN Val: OLD\")\n",
    "    data_valid_en = pd.read_csv('datasets/validgold_english_data_new.txt', sep=\"\\t\", header=None, usecols=[0,1,2])\n",
    "    data_valid_en.columns = [\"Word1\", \"Word2\", \"Label\"]\n",
    "\n",
    "else:\n",
    "    print(\"EN Val: OLD\")\n",
    "    data_valid_en = pd.read_csv('datasets/validgold_english_data.txt', sep=\"\\t\", header=None)\n",
    "    data_valid_en.columns = [\"Word1\", \"Word2\", \"Label\"]\n",
    "\n",
    "data_train_en[\"Label\"]=le.transform(data_train_en[\"Label\"])\n",
    "data_valid_en[\"Label\"]=le.transform(data_valid_en[\"Label\"])\n",
    "\n",
    "#german\n",
    "if \"de_train_new\" in datasets:\n",
    "    print(\"DE Train: NEW\")\n",
    "    #Cogalex 2.0 - NO DUPLICATES\n",
    "    data_train_de = pd.read_csv('datasets/train_german_data_new.txt', sep=\"\\t\", header=None, usecols=[0,1,2])\n",
    "    data_train_de.columns = [\"Word1\", \"Word2\", \"Label\"] \n",
    "else:\n",
    "    print(\"DE Train: OLD\")\n",
    "    # Old data\n",
    "    data_train_de = pd.read_csv('datasets/train_german_data.txt', sep=\"\\t\", header=None)\n",
    "    data_train_de.columns = [\"Word1\", \"Word2\", \"Label\"]\n",
    "\n",
    "if \"de_val_new\" in datasets:\n",
    "    print(\"DE Val: NEW\")\n",
    "    #Cogalex 2.0 - NO DUPLICATES\n",
    "    data_valid_de = pd.read_csv('datasets/validgold_german_data_new.txt', sep=\"\\t\", header=None, usecols=[0,1,2])\n",
    "    data_valid_de.columns = [\"Word1\", \"Word2\", \"Label\"]\n",
    "else:\n",
    "    print(\"DE Val: OLD\")\n",
    "    # Old data\n",
    "    data_valid_de = pd.read_csv('datasets/validgold_german_data.txt', sep=\"\\t\", header=None)\n",
    "    data_valid_de.columns = [\"Word1\", \"Word2\", \"Label\"]\n",
    "\n",
    "\n",
    "data_train_de[\"Label\"]=le.transform(data_train_de[\"Label\"])\n",
    "data_valid_de[\"Label\"]=le.transform(data_valid_de[\"Label\"])\n",
    "\n",
    "\n",
    "# all together\n",
    "data_train_all=pd.concat([data_train_zh, data_train_en, data_train_de])\n",
    "data_train_all=data_train_all.reset_index(drop=True)\n",
    "data_valid_all=pd.concat([data_valid_zh, data_valid_en, data_valid_de])\n",
    "data_valid_all=data_valid_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-colony",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "gQEd0yn6prGa",
    "outputId": "4fdd6fac-53e1-4049-8851-7b7d339e2dce",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-wells",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "ccHj8gEmqA88",
    "outputId": "21ac8e23-19a1-40cc-8534-1833efc38e99",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print class distribution \n",
    "data_train_all[\"Label\"].value_counts().plot(kind='bar', title='Count (target)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-seeker",
   "metadata": {
    "id": "da8D7C2WZDAh",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-navigator",
   "metadata": {
    "id": "zd6-b2-SiCiL",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Tokenize Data\n",
    "\n",
    "- encode in the right format for XLM-Roberta ( <s\\> = sentence beginning, </s\\> end of sentence/sentence seperator)\n",
    "\n",
    "- Truncate/Padding so everything has the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-behalf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoUK_mvmh1im",
    "outputId": "722962b2-a9fd-44ae-b4d3-5d12f67d4206",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "if \"gpt2\" in model_name:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-binary",
   "metadata": {
    "id": "fCoGBdK6iput",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_len=64\n",
    "\n",
    "def tokenizer_xlm(data, max_len):\n",
    "  labels_ = []\n",
    "  input_ids_ = []\n",
    "  attn_masks_ = []\n",
    "\n",
    "  # for each datasample:\n",
    "  for index, row in data.iterrows():\n",
    "\n",
    "      word1 = row['Word1']\n",
    "      word2 = row['Word2']\n",
    "\n",
    "      # create requiered input, i.e. ids and attention masks\n",
    "      encoded_dict = tokenizer.encode_plus(word1, word2, \n",
    "                                                max_length=max_len, \n",
    "                                                padding='max_length',\n",
    "                                                truncation=True, \n",
    "                                                return_tensors='pt')\n",
    "\n",
    "      # add encoded sample to lists\n",
    "      input_ids_.append(encoded_dict['input_ids'])\n",
    "      attn_masks_.append(encoded_dict['attention_mask'])\n",
    "      labels_.append(row['Label'])\n",
    "      \n",
    "  # Convert each Python list of Tensors into a 2D Tensor matrix.\n",
    "  input_ids_ = torch.cat(input_ids_, dim=0)\n",
    "  attn_masks_ = torch.cat(attn_masks_, dim=0)\n",
    "\n",
    "  # labels to tensor\n",
    "  labels_ = torch.tensor(labels_)\n",
    "\n",
    "  print('Encoder finished. {:,} examples.'.format(len(labels_)))\n",
    "  return input_ids_, attn_masks_, labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-heading",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yY8PwUXnA7h-",
    "outputId": "ea359df2-27ea-474a-b2bf-381eedac60dc",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a small test to see how the tokenizer works\n",
    "w1 = \"Tiger\"\n",
    "w2 = \"Animal\"\n",
    "\n",
    "# Encode the two sentences together.\n",
    "encoded = tokenizer.encode_plus(w1, w2)\n",
    "\n",
    "# Print the IDs of the resulting tokens.\n",
    "print (\"Input IDs:      \", encoded['input_ids'])\n",
    "\n",
    "# Convert the token IDs back to strings so we can check them out.\n",
    "print (\"Tokens:         \", tokenizer.convert_ids_to_tokens(encoded['input_ids']))\n",
    "\n",
    "# The tokenizer returns an attention mask, which masks out PAD tokens. \n",
    "# Since we aren't doing any padding yet, the mask is just all 1s. \n",
    "print (\"\\nAttention Mask: \", encoded['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-strip",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VPQUkRbVRGV5",
    "outputId": "be280054-d617-4062-a726-6d2395bf849e",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_train_all.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-birthday",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6HQFvnxIRMrC",
    "outputId": "bdcbb1ed-5a4e-409c-cab6-3a7ed9e997aa",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_valid_all.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-publication",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rD54rHpy4OEm",
    "outputId": "080d98ec-a9d3-4d09-bbbe-cacda13f51cf",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenize data\n",
    "\n",
    "#all\n",
    "print(\"All\")\n",
    "input_ids_train_all, attn_masks_train_all, labels_train_all = tokenizer_xlm(data_train_all, max_len)\n",
    "input_ids_valid_all, attn_masks_valid_all, labels_valid_all = tokenizer_xlm(data_valid_all, max_len)\n",
    "\n",
    "#zh\n",
    "print(\"zh\")\n",
    "input_ids_train_zh, attn_masks_train_zh, labels_train_zh = tokenizer_xlm(data_train_zh, max_len)\n",
    "input_ids_valid_zh, attn_masks_valid_zh, labels_valid_zh = tokenizer_xlm(data_valid_zh, max_len)\n",
    "\n",
    "#en\n",
    "print(\"en\")\n",
    "input_ids_train_en, attn_masks_train_en, labels_train_en = tokenizer_xlm(data_train_en, max_len)\n",
    "input_ids_valid_en, attn_masks_valid_en, labels_valid_en = tokenizer_xlm(data_valid_en, max_len)\n",
    "\n",
    "#de\n",
    "print(\"de\")\n",
    "input_ids_train_de, attn_masks_train_de, labels_train_de = tokenizer_xlm(data_train_de, max_len)\n",
    "input_ids_valid_de, attn_masks_valid_de, labels_valid_de = tokenizer_xlm(data_valid_de, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-investigation",
   "metadata": {
    "id": "7K62ohYg00oH",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "\n",
    "#all\n",
    "tensor_data_train_all = TensorDataset(input_ids_train_all, attn_masks_train_all, labels_train_all)\n",
    "tensor_data_valid_all = TensorDataset(input_ids_valid_all, attn_masks_valid_all, labels_valid_all)\n",
    "#zh\n",
    "tensor_data_train_zh = TensorDataset(input_ids_train_zh, attn_masks_train_zh, labels_train_zh)\n",
    "tensor_data_valid_zh = TensorDataset(input_ids_valid_zh, attn_masks_valid_zh, labels_valid_zh)\n",
    "#en\n",
    "tensor_data_train_en = TensorDataset(input_ids_train_en, attn_masks_train_en, labels_train_en)\n",
    "tensor_data_valid_en = TensorDataset(input_ids_valid_en, attn_masks_valid_en, labels_valid_en)\n",
    "#de\n",
    "tensor_data_train_de = TensorDataset(input_ids_train_de, attn_masks_train_de, labels_train_de)\n",
    "tensor_data_valid_de = TensorDataset(input_ids_valid_de, attn_masks_valid_de, labels_valid_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-cleaners",
   "metadata": {
    "id": "SRjNGp6Y1R_h",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prepare pytorch dataloaders\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#all\n",
    "train_dataloader_all = DataLoader(tensor_data_train_all, sampler = RandomSampler(tensor_data_train_all), batch_size = batch_size) #random sampling\n",
    "validation_dataloader_all = DataLoader(tensor_data_valid_all, sampler = SequentialSampler(tensor_data_valid_all),batch_size = batch_size ) #sequential sampling\n",
    "#zh\n",
    "train_dataloader_zh = DataLoader(tensor_data_train_zh, sampler = RandomSampler(tensor_data_train_zh), batch_size = batch_size)\n",
    "validation_dataloader_zh = DataLoader(tensor_data_valid_zh, sampler = SequentialSampler(tensor_data_valid_zh),batch_size = batch_size)\n",
    "#en\n",
    "train_dataloader_en = DataLoader(tensor_data_train_en, sampler = RandomSampler(tensor_data_train_en), batch_size = batch_size)\n",
    "validation_dataloader_en = DataLoader(tensor_data_valid_en, sampler = SequentialSampler(tensor_data_valid_en),batch_size = batch_size)\n",
    "#de\n",
    "train_dataloader_de = DataLoader(tensor_data_train_de, sampler = RandomSampler(tensor_data_train_de), batch_size = batch_size)\n",
    "validation_dataloader_de = DataLoader(tensor_data_valid_de, sampler = SequentialSampler(tensor_data_valid_de),batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-california",
   "metadata": {
    "id": "OAm6GWa-6NJS",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Training the Classifier (Finetunig the Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-portfolio",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-1l4-Ze6U5P",
    "outputId": "56fcaf37-6718-47c7-f84b-92819620b97d",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load the pretrained model provided by HuggingFace with an added untrained classification head for 4 classes\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "#model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-petroleum",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkxluZ1ulxeF",
    "outputId": "09fcc9a8-447d-4712-b703-7a53989840e5",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#choose training set used for this single training example (original training data for final model)\n",
    "train_dataloader = train_dataloader_all\n",
    "print(\"Training Samples:\",len(train_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-mapping",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rj2KvpZ46wJX",
    "outputId": "d297e160-cf80-42c6-b1e1-343691ea3caf",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# connect to GPU \n",
    "device = torch.device('cuda')\n",
    "# copy weights onto gpu\n",
    "desc = model.to(device)\n",
    "\n",
    "print('Connected to GPU:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-tomorrow",
   "metadata": {
    "id": "8znTmQ-067np",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Set parameters (learning rate & epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-better",
   "metadata": {
    "id": "zF-StfZ664Fj",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,   #do work well: 2e-5 with 5-7 epochs for trainall, 1e-5\n",
    "                  eps = 1e-8   # 1e-8.\n",
    "                  # weight_decay = 0          \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-enzyme",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3n400X88kau",
    "outputId": "a8472cb9-f289-4dcd-b39e-a984a85adebd",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of training epochs\n",
    "epochs = 15\n",
    "\n",
    "# number of batches x epochs\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "print(\"total steps:\", total_steps)\n",
    "\n",
    "#scheduler for lr\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,   #start low and increase learning rate during these steps\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-editor",
   "metadata": {
    "id": "Ia0s_Z4I3KO7",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Formatting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-explosion",
   "metadata": {
    "id": "u-OAa6pi5FVQ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-indiana",
   "metadata": {
    "id": "Rfz7OkuwEAXa",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-preserve",
   "metadata": {
    "id": "-66H7RWhDtti",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(validation_dataloader, model, verbose): \n",
    "  \n",
    "  # put model in evaluation mode \n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  total_eval_loss = 0\n",
    "\n",
    "\n",
    "  predictions, true_labels = [], []\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "          \n",
    "      # Unpack training batch and copy the tensors to the gpu\n",
    "      b_input_ids = batch[0].to(device)\n",
    "      b_input_mask = batch[1].to(device)\n",
    "      b_labels = batch[2].to(device)\n",
    "          \n",
    "      # no backprop needed\n",
    "      with torch.no_grad():        \n",
    "\n",
    "          # forward pass\n",
    "          if \"distilbert\" in model.name_or_path:\n",
    "              output = model(b_input_ids, \n",
    "                              attention_mask=b_input_mask, \n",
    "                              labels=b_labels)\n",
    "          else:\n",
    "              output = model(b_input_ids, \n",
    "                              token_type_ids=None, \n",
    "                              attention_mask=b_input_mask, \n",
    "                              labels=b_labels)\n",
    "          \n",
    "          loss=output.loss\n",
    "          logits=output.logits\n",
    "              \n",
    "      # add up loss\n",
    "      total_eval_loss += loss.item()\n",
    "\n",
    "      # on cpu\n",
    "      logits = logits.detach().cpu().numpy()\n",
    "      label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "      # save preds/true labels\n",
    "      predictions.append(logits)\n",
    "      true_labels.append(label_ids)\n",
    "\n",
    "  # results of the whole validation set\n",
    "  flat_predictions = np.concatenate(predictions, axis=0)\n",
    "  flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "  # logit to label\n",
    "  predicted_labels = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "  # print classification report\n",
    "  if verbose:\n",
    "    print(classification_report(flat_true_labels, predicted_labels, target_names=labels))\n",
    "\n",
    "  # Calculate the validation accuracy, macro f1, and weighted f1 without RANDOM\n",
    "  val_accuracy = (predicted_labels == flat_true_labels).mean()\n",
    "  macroF1 = f1_score(flat_true_labels, predicted_labels, average='macro')\n",
    "  weightedF1=f1_score(flat_true_labels, predicted_labels, average='weighted')\n",
    "  print(\"\\t Weighted F1 (no random):\", weightedF1)\n",
    "\n",
    "  # Calculate the average loss over all of the batches.\n",
    "  avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "  # plot confusion matrix\n",
    "  if verbose:\n",
    "    print(confusion_matrix(flat_true_labels, predicted_labels, labels=[0,1,2,3]))\n",
    "\n",
    "  return avg_val_loss, val_accuracy, macroF1, weightedF1\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-riverside",
   "metadata": {
    "id": "CrCKhOce9ASW",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-fitting",
   "metadata": {
    "id": "BcgjT8Gw82UN",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(epochs, model, train_dataloader, validation_dataloader_set, random_seed, verbose, optimizer, scheduler):\n",
    "\n",
    "  seed_val = random_seed\n",
    "\n",
    "  random.seed(seed_val)\n",
    "  np.random.seed(seed_val)\n",
    "  torch.manual_seed(seed_val)\n",
    "  torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "  # mostly contains scores about how the training went for each epoch\n",
    "  training_stats = []\n",
    "\n",
    "  # total training time\n",
    "  total_t0 = time.time()\n",
    "\n",
    "  print('\\033[1m'+\"================ Model Training ================\"+'\\033[0m')\n",
    "\n",
    "  # For each epoch...\n",
    "  for epoch_i in range(0, epochs):\n",
    "\n",
    "      print(\"\")\n",
    "      print('\\033[1m'+'======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs)+'\\033[0m')\n",
    "\n",
    "      t0 = time.time()\n",
    "\n",
    "      # summed training loss of the epoch\n",
    "      total_train_loss = 0\n",
    "\n",
    "\n",
    "      # model is being put into training mode as mechanisms like dropout work differently during train and test time\n",
    "      model.train()\n",
    "\n",
    "      # iterrate over batches\n",
    "      for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "          # unpack training batch at load it to gpu (device)  \n",
    "          b_input_ids = batch[0].to(device)\n",
    "          b_input_mask = batch[1].to(device)\n",
    "          b_labels = batch[2].to(device)\n",
    "\n",
    "          # clear gradients before calculating new ones\n",
    "          model.zero_grad()        \n",
    "\n",
    "          # forward pass with current batch\n",
    "          if \"distilbert\" in model.name_or_path:\n",
    "              output = model(b_input_ids, \n",
    "                              attention_mask=b_input_mask, \n",
    "                              labels=b_labels)\n",
    "          else:\n",
    "              output = model(b_input_ids, \n",
    "                              token_type_ids=None, \n",
    "                              attention_mask=b_input_mask, \n",
    "                              labels=b_labels)\n",
    "          \n",
    "          loss=output.loss\n",
    "          logits=output.logits\n",
    "\n",
    "          # add up the loss\n",
    "          total_train_loss += loss.item()\n",
    "\n",
    "          # calculate new gradients\n",
    "          loss.backward()\n",
    "\n",
    "          # gradient clipping (not bigger than)\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "          # Update the networks weights based on the gradient as well as the optimiziers parameters\n",
    "          optimizer.step()\n",
    "\n",
    "          # lr update\n",
    "          scheduler.step()\n",
    "\n",
    "      # avg loss over all batches\n",
    "      avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "      \n",
    "      # training time of this epoch\n",
    "      training_time = format_time(time.time() - t0)\n",
    "\n",
    "      print(\"\")\n",
    "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "      print(\"  Training epoch took: {:}\".format(training_time))\n",
    "          \n",
    "  \n",
    "      # VALIDATION\n",
    "\n",
    "      #all\n",
    "      print(\"evaluate on all\")\n",
    "      avg_val_loss_all, val_accuracy_all, macroF1_all, weightedF1_no_random_all = validate(validation_dataloader_set[0], model, verbose)   \n",
    "      #zh\n",
    "      print(\"evaluate on zh\")\n",
    "      avg_val_loss_zh, val_accuracy_zh, macroF1_zh, weightedF1_no_random_zh = validate(validation_dataloader_set[1], model, verbose) \n",
    "      #en\n",
    "      print(\"evaluate on en\")\n",
    "      avg_val_loss_en, val_accuracy_en, macroF1_en, weightedF1_no_random_en = validate(validation_dataloader_set[2], model, verbose) \n",
    "      #de\n",
    "      print(\"evaluate on de\")\n",
    "      avg_val_loss_de, val_accuracy_de, macroF1_de, weightedF1_no_random_de = validate(validation_dataloader_set[3], model, verbose)  \n",
    "       \n",
    "\n",
    "      print('\\033[1m'+ \"  Validation Loss All: {0:.2f}\".format(avg_val_loss_all) + '\\033[0m')\n",
    "\n",
    "      training_stats.append(\n",
    "          {\n",
    "              'epoch': epoch_i + 1,\n",
    "              'Training Loss': avg_train_loss,\n",
    "              'Valid. Loss all': avg_val_loss_all,\n",
    "              'Valid. Accur. all': val_accuracy_all,\n",
    "              'Weigh_F1 all (no random)': weightedF1_no_random_all, \n",
    "              'Macro F1 all': macroF1_all,\n",
    "              'Weigh_F1 en': weightedF1_no_random_en,\n",
    "              'Weigh_F1 de': weightedF1_no_random_de,\n",
    "              'Weigh_F1 zh': weightedF1_no_random_zh,\n",
    "              'Training Time': training_time,\n",
    "          }\n",
    "      )\n",
    "\n",
    "  print(\"\\n\\nTraining complete!\")\n",
    "  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "  \n",
    "  return training_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-links",
   "metadata": {
    "id": "dz5D5PXzwxjt",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_dataloader_set=[validation_dataloader_all, validation_dataloader_zh, validation_dataloader_en, validation_dataloader_de]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-sperm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dsue5wnCmQCr",
    "outputId": "0f5039fc-c4b6-4713-baa8-9429a49fdffa",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "training_stats=train_model(epochs=epochs, \n",
    "                           model=model, \n",
    "                           train_dataloader=train_dataloader, \n",
    "                           validation_dataloader_set=validation_dataloader_set,\n",
    "                           random_seed=42,\n",
    "                           verbose=True,\n",
    "                           scheduler=scheduler,\n",
    "                           optimizer=optimizer)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-restriction",
   "metadata": {
    "id": "-fjiw0rymj7U",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Evaluation Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-classification",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "id": "ICm4zOH-Q5p2",
    "outputId": "f136892d-0825-4d27-d771-e96f8118e89a",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Table with training stats\n",
    "pd.set_option('precision', 3)\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-theta",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "lXGHvyQMQ8vB",
    "outputId": "48c02348-50d6-4a57-b339-f488a3e8f35e",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot Trainingloss, Validation loss and Weighted F1 score for each epoch\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss all'], 'g-o', label=\"Validation\")\n",
    "plt.plot(df_stats['Weigh_F1 all (no random)'], 'r-o', label=\"Weighted F1 (all, no rand)\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-expansion",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzwvY1JTfjv2",
    "outputId": "aa62fdfb-3d6b-40ec-9787-fb4763ad1ff6",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "import os\n",
    "if g_colab:\n",
    "    workdir = os.getcwd()\n",
    "else:\n",
    "    workdir = os.getcwd().replace(\"/home/\",\"/binfl/\")\n",
    "\n",
    "print(workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-sacramento",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUwmJJVeBunw",
    "outputId": "c4806049-21d1-4f84-b30e-edd755b3b797",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(f\"{workdir}/saved_models/{model_name}_{it}\", exist_ok=True)\n",
    "os.makedirs(\"./stats/\", exist_ok=True)\n",
    "PATH = f\"{workdir}/saved_models/{model_name}_{it}\"\n",
    "df_stats.to_csv(f\"./stats/{model_name}_{it}\")\n",
    "model.save_pretrained(PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "CogALex 2.0 w/ grid-search",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.439222,
   "end_time": "2021-08-24T12:27:37.454417",
   "environment_variables": {},
   "exception": true,
   "input_path": "CogALex_2_0_w_grid_search.ipynb",
   "output_path": "/home/lv71502/clang/Cogalex2.0/2021-08-24/2021-08-24_CogALex_2_0_w_grid_search_8.ipynb",
   "parameters": {
    "it": 7,
    "rdate": "2021-08-24"
   },
   "start_time": "2021-08-24T12:27:28.015195",
   "version": "2.3.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}